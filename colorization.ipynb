{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fe8a06b-77c1-40b4-8c26-5d4da8a8a166",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import itertools\n",
    "from matplotlib import image\n",
    "import glob as glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from skimage import color\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae408294-7360-44aa-8868-28333470680c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the CPU. Overall speed may be slowed down\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU. You are good to go!\")\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print(\"Using the CPU. Overall speed may be slowed down\")\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3653e0f3",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6397db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecolorData(Dataset):\n",
    "  def __init__(self, root_dir, split='train', transform=None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        root_dir: the directory of the dataset\n",
    "        split: \"train\", \"valid\", or \"test\"\n",
    "        transform: pytorch transformations.\n",
    "    \"\"\"\n",
    "\n",
    "    self.transform = transform\n",
    "\n",
    "    self.files = glob.glob(os.path.join(root_dir, split, '*.jpg'))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.files)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    img = Image.open(self.files[idx])\n",
    "    img = color.rgb2lab(img)\n",
    "    img = np.asarray(img)\n",
    "    if self.transform:\n",
    "        img = self.transform(img)\n",
    "    return img[0:1, :, :], img[1:3, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00c76231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images 13492, number of validation images 500, number of testing images 500\n"
     ]
    }
   ],
   "source": [
    "# Define transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((64, 64)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize the images\n",
    "])\n",
    "\n",
    "# Define the directory containing your image folders\n",
    "data_dir = 'dataset128'\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "train_dataset = RecolorData(data_dir, split=\"train\", transform=transform)\n",
    "valid_dataset = RecolorData(data_dir, split=\"valid\", transform=transform)\n",
    "test_dataset = RecolorData(data_dir, split=\"test\", transform=transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Create a DataLoader to iterate over the dataset\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print('Number of training images {}, number of validation images {}, number of testing images {}'.format(len(train_dataset), len(valid_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "833c1eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128, 128])\n",
      "torch.Size([32, 2, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "for batch, label in trainloader:\n",
    "    print(batch.shape)\n",
    "    print(label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f8b65",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc325e6",
   "metadata": {},
   "source": [
    "objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5e0b1364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(window_size, sigma):\n",
    "    \"\"\"\n",
    "    Generates a list of Tensor values drawn from a gaussian distribution with standard\n",
    "    diviation = sigma and sum of all elements = 1.\n",
    "\n",
    "    Length of list = window_size\n",
    "    \"\"\"    \n",
    "    gauss =  torch.Tensor([math.exp(-(x - window_size//2)**2/float(2*sigma**2)) for x in range(window_size)])\n",
    "    return gauss/gauss.sum()\n",
    "\n",
    "def create_window(window_size, channel=1):\n",
    "\n",
    "    # Generate an 1D tensor containing values sampled from a gaussian distribution\n",
    "    _1d_window = gaussian(window_size=window_size, sigma=1.5).unsqueeze(1)\n",
    "    \n",
    "    # Converting to 2D  \n",
    "    _2d_window = _1d_window.mm(_1d_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "     \n",
    "    window = torch.Tensor(_2d_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "\n",
    "    return window\n",
    "\n",
    "def ssim(img1, img2, val_range=255, window_size=11, window=None, size_average=True, full=False):\n",
    "\n",
    "    L = val_range # L is the dynamic range of the pixel values (255 for 8-bit grayscale images),\n",
    "\n",
    "    pad = window_size // 2\n",
    "    \n",
    "    try:\n",
    "        _, channels, height, width = img1.size()\n",
    "    except:\n",
    "        channels, height, width = img1.size()\n",
    "\n",
    "    # if window is not provided, init one\n",
    "    if window is None: \n",
    "        real_size = min(window_size, height, width) # window should be atleast 11x11 \n",
    "        window = create_window(real_size, channel=channels).to(img1.device)\n",
    "    \n",
    "    # calculating the mu parameter (locally) for both images using a gaussian filter \n",
    "    # calculates the luminosity params\n",
    "    mu1 = F.conv2d(img1, window, padding=pad, groups=channels)\n",
    "    mu2 = F.conv2d(img2, window, padding=pad, groups=channels)\n",
    "    \n",
    "    mu1_sq = mu1 ** 2\n",
    "    mu2_sq = mu2 ** 2 \n",
    "    mu12 = mu1 * mu2\n",
    "\n",
    "    # now we calculate the sigma square parameter\n",
    "    # Sigma deals with the contrast component \n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=pad, groups=channels) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=pad, groups=channels) - mu2_sq\n",
    "    sigma12 =  F.conv2d(img1 * img2, window, padding=pad, groups=channels) - mu12\n",
    "\n",
    "    # Some constants for stability \n",
    "    C1 = (0.01 ) ** 2  # NOTE: Removed L from here (ref PT implementation)\n",
    "    C2 = (0.03 ) ** 2 \n",
    "\n",
    "    contrast_metric = (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)\n",
    "    contrast_metric = torch.mean(contrast_metric)\n",
    "\n",
    "    numerator1 = 2 * mu12 + C1  \n",
    "    numerator2 = 2 * sigma12 + C2\n",
    "    denominator1 = mu1_sq + mu2_sq + C1 \n",
    "    denominator2 = sigma1_sq + sigma2_sq + C2\n",
    "\n",
    "    ssim_score = (numerator1 * numerator2) / (denominator1 * denominator2)\n",
    "\n",
    "    if size_average:\n",
    "        ret = ssim_score.mean() \n",
    "    else: \n",
    "        ret = ssim_score.mean(1).mean(1).mean(1)\n",
    "    \n",
    "    if full:\n",
    "        return ret, contrast_metric\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c755668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ##############################################################################\n",
    "        # TODO: Design your own network, define layers here.                          #\n",
    "        # Here We provide a sample of two-layer fc network from HW4 Part3.           #\n",
    "        # Your solution, however, should contain convolutional layers.               #\n",
    "        # Refer to PyTorch documentations of torch.nn to pick your layers.           #\n",
    "        # (https://pytorch.org/docs/stable/nn.html)                                  #\n",
    "        # Some common choices: Linear, Conv2d, ReLU, MaxPool2d, AvgPool2d, Dropout   #\n",
    "        # If you have many layers, use nn.Sequential() to simplify your code         #\n",
    "        ##############################################################################\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        # self.layer3 = nn.Linear(32*7*7,10)\n",
    "\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        ##############################################################################\n",
    "        # TODO: Design your own network, implement forward pass here                 #\n",
    "        ##############################################################################\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        # breakpoint()\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        return x\n",
    "        ##############################################################################\n",
    "        #                             END OF YOUR CODE                               #\n",
    "        ##############################################################################\n",
    "\n",
    "model = Network().to(device)\n",
    "criterion = ssim # Specify the loss layer\n",
    "print('Your network:')\n",
    "print(summary(model, (1,28,28), device=device)) # visualize your model\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Modify the lines below to experiment with different optimizers,      #\n",
    "# parameters (such as learning rate) and number of epochs.                   #\n",
    "##############################################################################\n",
    "# Set up optimization hyperparameters\n",
    "learning_rate, weight_decay, num_epoch = .001, 0, 14\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe062ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def train(model, trainloader, validloader, num_epoch=10):  # Train the model\n",
    "    print(\"Start training...\")\n",
    "    trn_loss_hist = []\n",
    "    trn_acc_hist = []\n",
    "    val_acc_hist = []\n",
    "    model.train()  # Set the model to training mode\n",
    "    for i in range(num_epoch):\n",
    "        running_loss = []\n",
    "        print('-----------------Epoch = %d-----------------' % (i+1))\n",
    "        for batch, label in tqdm(trainloader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad()  # Clear gradients from the previous iteration\n",
    "            # This will call Network.forward() that you implement\n",
    "            pred = model(batch)\n",
    "            loss = criterion(pred, label)  # Calculate the loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward()  # Backprop gradients to all tensors in the network\n",
    "            optimizer.step()  # Update trainable weights\n",
    "        print(\"\\n Epoch {} loss:{}\".format(i+1, np.mean(running_loss)))\n",
    "\n",
    "        # Keep track of training loss, accuracy, and validation loss\n",
    "        trn_loss_hist.append(np.mean(running_loss))\n",
    "        trn_acc_hist.append(evaluate(model, trainloader))\n",
    "        print(\"\\n Evaluate on validation set...\")\n",
    "        val_acc_hist.append(evaluate(model, validloader))\n",
    "    print(\"Done!\")\n",
    "    return trn_loss_hist, trn_acc_hist, val_acc_hist\n",
    "\n",
    "\n",
    "def evaluate(model, loader):  # Evaluate accuracy on validation / test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    with torch.no_grad():  # Do not calculate grident to speed up computation\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(batch)\n",
    "            correct += (torch.argmax(pred, dim=1) == label).sum().item()\n",
    "        acc = correct/len(loader.dataset)\n",
    "        print(\"\\n Evaluation accuracy: {}\".format(acc))\n",
    "        return acc\n",
    "\n",
    "\n",
    "trn_loss_hist, trn_acc_hist, val_acc_hist = train(model, trainloader,\n",
    "                                                  validloader, num_epoch)\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Note down the evaluation accuracy on test set                        #\n",
    "##############################################################################\n",
    "print(\"\\n Evaluate on test set\")\n",
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# TODO: Submit the accuracy plot                                             #\n",
    "##############################################################################\n",
    "# visualize the training / validation accuracies\n",
    "x = np.arange(num_epoch)\n",
    "# train/val accuracies for MiniVGG\n",
    "plt.figure()\n",
    "plt.plot(x, trn_acc_hist)\n",
    "plt.plot(x, val_acc_hist)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.xticks(x)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('fashion MNIST Classification')\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.savefig('part1.png', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
